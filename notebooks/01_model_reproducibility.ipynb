{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Reproducibility\n",
    "\n",
    "In this notebook I will reproduce one of the examples from the publication associated to the model and make sure the Ersilia Model Hub implementation is giving the same results.The test is explained in the ReadMe File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from ersilia import ErsiliaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset \n",
    "test_set_1 = pd.read_csv(\"/home/sharonatieno/model-reproducibility/data/external_test_set_pos.csv\") # test set 1\n",
    "test_set_2 = pd.read_csv(\"/home/sharonatieno/model-reproducibility/data/external_test_set_neg.csv\") # test set 2\n",
    "test_set_3 = pd.read_csv(\"/home/sharonatieno/model-reproducibility/data/external_test_set_new.csv\") # test set \n",
    "\n",
    "# load predictions\n",
    "pred_test_set_1 = pd.read_csv(\"/home/sharonatieno/model-reproducibility/data/pred_external_test_set_pos.csv\") # predictions test set 1\n",
    "pred_test_set_2 = pd.read_csv(\"/home/sharonatieno/model-reproducibility/data/pred_external_test_set_neg.csv\") # predictions test set 2\n",
    "pred_test_set_3 = pd.read_csv(\"/home/sharonatieno/model-reproducibility/data/pred_external_test_set_new.csv\") # predictions test set 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set 1 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set 1 Evaluation Metrics\n",
      "***********************************************\n",
      "***********************************************\n",
      "auc_test_external_stack_pos:0.8095\n",
      "specificity_test_external_stack_pos:0.7857\n",
      "sensitivity_test_external_stack_pos:0.8333\n",
      "NPV_test_external_stack_pos:0.6875\n",
      "PPV_test_external_stack_pos:0.8929\n",
      "Accuracy_test_external_stack_pos:0.8095\n",
      "MCC_test_external_stack_pos:0.5994\n"
     ]
    }
   ],
   "source": [
    "# assign y_true_1 to activity column\n",
    "y_true_1 = test_set_1[\"ACTIVITY\"]\n",
    "\n",
    "# create column\n",
    "# that evaluates to 1 if float is greater than 0.5 \n",
    "# and 0 if not\n",
    "# values with 1 are hERG blockers\n",
    "# values with 0 are non hERG blockers\n",
    "pred_test_set_1[\"pred_1\"] = pred_test_set_1['predictions'].apply(lambda x : 1 if x > 0.5 else 0)\n",
    "\n",
    "# column pred_1 holds predictions\n",
    "# assign the column to y_pred_1\n",
    "y_pred_1 = pred_test_set_1['pred_1']\n",
    "\n",
    "# compute roc_auc score for test set 1 \n",
    "test_set_1_score = roc_auc_score(y_true_1, y_pred_1)\n",
    "\n",
    "# create numpy arrays to be fed into the confusion matrix\n",
    "y_true_array_1 = y_true_1.values\n",
    "y_pred_array_1 = y_pred_1.values\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true_array_1, y_pred_array_1).ravel()\n",
    "\n",
    "specificity_test_external_stack_pos = tn / (tn + fp)\n",
    "sensitivity_test_external_stack_pos = tp / (tp + fn)\n",
    "\n",
    "NPV_test_external_stack_pos = tn / (tn + fn)\n",
    "PPV_test_external_stack_pos = tp / (tp + fp)\n",
    "\n",
    "Accuracy_test_external_stack_pos = balanced_accuracy_score(y_true_1, y_pred_1) \n",
    "\n",
    "MCC_test_external_stack_pos = matthews_corrcoef(y_true_1, y_pred_1)\n",
    "\n",
    "print(\"Test Set 1 Evaluation Metrics\")\n",
    "print(\"***********************************************\")\n",
    "print(\"***********************************************\")\n",
    "print(f\"auc_test_external_stack_pos:{round(test_set_1_score, 4)}\")\n",
    "print(f\"specificity_test_external_stack_pos:{round(specificity_test_external_stack_pos, 4)}\")\n",
    "print(f\"sensitivity_test_external_stack_pos:{round(sensitivity_test_external_stack_pos, 4)}\")\n",
    "print(f\"NPV_test_external_stack_pos:{round(NPV_test_external_stack_pos, 4)}\")\n",
    "print(f\"PPV_test_external_stack_pos:{round(PPV_test_external_stack_pos, 4)}\")\n",
    "print(f\"Accuracy_test_external_stack_pos:{round(Accuracy_test_external_stack_pos, 4)}\")\n",
    "print(f\"MCC_test_external_stack_pos:{round(MCC_test_external_stack_pos, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set 2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set 2 Evaluation Metrics\n",
      "***********************************\n",
      "***********************************\n",
      "auc_test_external_stack_neg:0.7545\n",
      "specificity_test_external_stack_neg:0.6\n",
      "sensitivity_test_external_stack_neg:0.9091\n",
      "NPV_test_external_stack_neg:0.9474\n",
      "PPV_test_external_stack_neg:0.4545\n",
      "Accuracy_test_external_stack_neg:0.7545\n",
      "MCC_test_external_stack_neg:0.4523\n"
     ]
    }
   ],
   "source": [
    "# assign y_true_2 to activity column\n",
    "y_true_2 = test_set_2[\"ACTIVITY\"]\n",
    "\n",
    "# create column\n",
    "# that evaluates to 1 if float is greater than 0.5 \n",
    "# and 0 if not\n",
    "# values with 1 are hERG blockers\n",
    "# values with 0 are non hERG blockers\n",
    "pred_test_set_2[\"pred_2\"] = pred_test_set_2['predictions'].apply(lambda x : 1 if x > 0.5 else 0)\n",
    "\n",
    "# column pred_2 holds predictions\n",
    "# assign the column to y_pred_2\n",
    "y_pred_2 = pred_test_set_2['pred_2']\n",
    "\n",
    "# compute roc_auc score for test set 2 \n",
    "test_set_2_score = roc_auc_score(y_true_2, y_pred_2)\n",
    "\n",
    "# create numpy arrays to be fed into the confusion matrix\n",
    "y_true_2_array = y_true_2.values\n",
    "y_pred_2_array = y_pred_2.values\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true_2_array, y_pred_2_array).ravel()\n",
    "\n",
    "specificity_test_external_stack_neg = tn / (tn + fp)\n",
    "sensitivity_test_external_stack_neg = tp / (tp + fn)\n",
    "\n",
    "NPV_test_external_stack_neg = tn / (tn + fn)\n",
    "PPV_test_external_stack_neg = tp / (tp + fp)\n",
    "\n",
    "Accuracy_test_external_stack_neg = balanced_accuracy_score(y_true_2, y_pred_2)\n",
    "\n",
    "MCC_test_external_stack_neg = matthews_corrcoef(y_true_2, y_pred_2)\n",
    "\n",
    "print(\"Test Set 2 Evaluation Metrics\")\n",
    "print(\"*****************************************\")\n",
    "print(\"*****************************************\")\n",
    "print(f\"auc_test_external_stack_neg:{round(test_set_2_score, 4)}\")\n",
    "print(f\"specificity_test_external_stack_neg:{round(specificity_test_external_stack_neg, 4)}\")\n",
    "print(f\"sensitivity_test_external_stack_neg:{round(sensitivity_test_external_stack_neg, 4)}\")\n",
    "print(f\"NPV_test_external_stack_neg:{round(NPV_test_external_stack_neg, 4)}\")\n",
    "print(f\"PPV_test_external_stack_neg:{round(PPV_test_external_stack_neg, 4)}\")\n",
    "print(f\"Accuracy_test_external_stack_neg:{round(Accuracy_test_external_stack_neg, 4)}\")\n",
    "print(f\"MCC_test_external_stack_neg:{round(MCC_test_external_stack_neg, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set 3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set 3 Evaluation Metrics\n",
      "***************************************\n",
      "***************************************\n",
      "auc_test_external_stack_new:0.7462\n",
      "specificity_test_external_stack_new:0.6983\n",
      "sensitivity_test_external_stack_new:0.7941\n",
      "NPV_test_external_stack_new:0.986\n",
      "PPV_test_external_stack_new:0.1125\n",
      "Accuracy_test_external_stack_new:0.7462\n",
      "MCC_test_external_stack_new:0.2202\n"
     ]
    }
   ],
   "source": [
    "# assign y_true_3 to activity column\n",
    "y_true_3 = test_set_3[\"ACTIVITY\"]\n",
    "\n",
    "# create column\n",
    "# that evaluates to 1 if float is greater than 0.5 \n",
    "# and 0 if not\n",
    "# values with 1 are hERG blockers\n",
    "# values with 0 are non hERG blockers\n",
    "pred_test_set_3[\"pred_3\"] = pred_test_set_3['predictions'].apply(lambda x : 1 if x > 0.5 else 0)\n",
    "\n",
    "# column pred_3 holds predictions\n",
    "# assign the column to y_pred_3\n",
    "y_pred_3 = pred_test_set_3['pred_3']\n",
    "\n",
    "# compute roc_auc score for external test set 3\n",
    "test_set_3_score = roc_auc_score(y_true_3, y_pred_3)\n",
    "\n",
    "# create numpy arrays to be fed into the confusion matrix\n",
    "y_true_3_array = y_true_3.values\n",
    "y_pred_3_array = y_pred_3.values\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true_3_array, y_pred_3_array).ravel()\n",
    "\n",
    "specificity_test_external_stack_new = tn / (tn + fp)\n",
    "sensitivity_test_external_stack_new = tp / (tp + fn)\n",
    "\n",
    "NPV_test_external_stack_new = tn / (tn + fn)\n",
    "PPV_test_external_stack_new = tp / (tp + fp)\n",
    "\n",
    "Accuracy_test_external_stack_new = balanced_accuracy_score(y_true_3, y_pred_3)\n",
    "\n",
    "MCC_test_external_stack_new = matthews_corrcoef(y_true_3, y_pred_3)\n",
    "\n",
    "print(\"Test Set 3 Evaluation Metrics\")\n",
    "print(\"***************************************\")\n",
    "print(\"***************************************\")\n",
    "print(f\"auc_test_external_stack_new:{round(test_set_3_score, 4)}\")\n",
    "print(f\"specificity_test_external_stack_new:{round(specificity_test_external_stack_new, 4)}\")\n",
    "print(f\"sensitivity_test_external_stack_new:{round(sensitivity_test_external_stack_new, 4)}\")\n",
    "print(f\"NPV_test_external_stack_new:{round(NPV_test_external_stack_new, 4)}\")\n",
    "print(f\"PPV_test_external_stack_new:{round(PPV_test_external_stack_new, 4)}\")\n",
    "print(f\"Accuracy_test_external_stack_new:{round(Accuracy_test_external_stack_new, 4)}\")\n",
    "print(f\"MCC_test_external_stack_new:{round(MCC_test_external_stack_new, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results for Test set 1, 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|    Data    |     Method    |  MCC   |  NPV   |  ACC   |  PPV   |  SPE   |  SEN   | B-ACC  |\n",
      "+------------+---------------+--------+--------+--------+--------+--------+--------+--------+\n",
      "| Test Set 1 | Cardiotox Net | 0.5994 | 0.6875 | 0.8095 | 0.8929 | 0.7857 | 0.8333 | 0.8095 |\n",
      "| Test Set 2 | Cardiotox Net | 0.4523 | 0.9474 | 0.7545 | 0.4545 |  0.6   | 0.9091 | 0.7545 |\n",
      "| Test Set 3 | Cardiotox Net | 0.2202 | 0.986  | 0.7462 | 0.1125 | 0.6983 | 0.7941 | 0.7462 |\n",
      "+------------+---------------+--------+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "eval_table = PrettyTable(['Data', \"Method\", \"MCC\",\"NPV\", \"ACC\", \"PPV\", \"SPE\", \"SEN\", \"B-ACC\"])\n",
    "eval_table.add_row([\"Test Set 1\", \"Cardiotox Net\", 0.5994, 0.6875, 0.8095, 0.8929, 0.7857, 0.8333, 0.8095])\n",
    "eval_table.add_row([\"Test Set 2\", \"Cardiotox Net\", 0.4523, 0.9474, 0.7545, 0.4545, 0.6, 0.9091, 0.7545])                  \n",
    "eval_table.add_row(['Test Set 3', \"Cardiotox Net\", 0.2202, 0.986, 0.7462, 0.1125, 0.6983, 0.7941, 0.7462])                   \n",
    "\n",
    "print(eval_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ersilia Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sudo: unknown user udockerusername\n",
      "sudo: error initializing audit plugin sudoers_audit\n",
      "sudo: unknown user udockerusername\n",
      "sudo: error initializing audit plugin sudoers_audit\n"
     ]
    }
   ],
   "source": [
    "# load test set 1\n",
    "test_set_1 = pd.read_csv(\"/home/sonia/ersilia-model-validation/data/external_test_set_pos.csv\")\n",
    "\n",
    "# SMILES pandas series \n",
    "test_set_1_smiles = test_set_1['smiles']\n",
    "\n",
    "# run test set 1 on eos2ta5\n",
    "# instantiate model\n",
    "eos2ta5_model = ErsiliaModel(\"eos2ta5\")\n",
    "\n",
    "# serve model\n",
    "eos2ta5_model.serve()\n",
    "\n",
    "# make predictions\n",
    "# have the predictions in a pandas daframe\n",
    "eos2ta5_output = eos2ta5_model.run(test_set_1_smiles, output=\"pandas\")\n",
    "\n",
    "# store predictions in a csv file\n",
    "eos2ta5_output.to_csv(\"/home/sonia/ersilia-model-validation/data/ersilia_pred_external_test_set_pos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load eos2ta5 model predictions\n",
    "# run on test set 1 \n",
    "ersilia_pred = pd.read_csv(\"/home/sonia/ersilia-model-validation/data/ersilia_pred_external_test_set_pos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ersilia Evaluation Metrics\n",
      "***********************************\n",
      "***********************************\n",
      "test_set_ersilia_score:0.8095\n",
      "specificity_test_external_stack_ersilia:0.7857\n",
      "sensitivity_test_external_stack_ersilia:0.8333\n",
      "NPV_test_external_stack_ersilia:0.6875\n",
      "PPV_test_external_stack_ersilia:0.8929\n",
      "Accuracy_test_external_stack_ersilia:0.8095\n",
      "MCC_test_external_stack_ersilia:0.5994\n"
     ]
    }
   ],
   "source": [
    "# assign y_true_4 to activity column\n",
    "y_true_4 = test_set_1[\"ACTIVITY\"]\n",
    "\n",
    "# create column\n",
    "# that evaluates to 1 if float is greater than 0.5 \n",
    "# and 0 if not\n",
    "# values with 1 are hERG blockers\n",
    "# values with 0 are non hERG blockers\n",
    "ersilia_pred[\"pred_ersilia\"] = ersilia_pred['probability'].apply(lambda x : 1 if x > 0.5 else 0)\n",
    "\n",
    "# column pred_ersilia holds eos2ta5 predictions\n",
    "# assign the column to y_pred_4\n",
    "y_pred_4 = ersilia_pred['pred_ersilia']\n",
    "\n",
    "# compute roc_auc score for test set 1 \n",
    "test_set_ersilia_score = roc_auc_score(y_true_4, y_pred_4)\n",
    "\n",
    "# create numpy arrays to be fed into the confusion matrix\n",
    "y_true_array_4 = y_true_4.values\n",
    "y_pred_array_4 = y_pred_4.values\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true_array_4, y_pred_array_4).ravel()\n",
    "\n",
    "specificity_test_external_stack_ersilia = tn / (tn + fp)\n",
    "sensitivity_test_external_stack_ersilia = tp / (tp + fn)\n",
    "\n",
    "NPV_test_external_stack_ersilia = tn / (tn + fn)\n",
    "PPV_test_external_stack_ersilia = tp / (tp + fp)\n",
    "\n",
    "Accuracy_test_external_stack_ersilia = balanced_accuracy_score(y_true_4, y_pred_4)\n",
    "\n",
    "MCC_test_external_stack_ersilia = matthews_corrcoef(y_true_4, y_pred_4)\n",
    "\n",
    "print(\"Ersilia Evaluation Metrics\")\n",
    "print(\"***********************************\")\n",
    "print(\"***********************************\")\n",
    "print(f\"test_set_ersilia_score:{round(test_set_ersilia_score, 4)}\")\n",
    "print(f\"specificity_test_external_stack_ersilia:{round(specificity_test_external_stack_ersilia, 4)}\")\n",
    "print(f\"sensitivity_test_external_stack_ersilia:{round(sensitivity_test_external_stack_ersilia, 4)}\")\n",
    "print(f\"NPV_test_external_stack_ersilia:{round(NPV_test_external_stack_ersilia, 4)}\")\n",
    "print(f\"PPV_test_external_stack_ersilia:{round(PPV_test_external_stack_ersilia, 4)}\")\n",
    "print(f\"Accuracy_test_external_stack_ersilia:{round(Accuracy_test_external_stack_ersilia, 4)}\")\n",
    "print(f\"MCC_test_external_stack_ersilia:{round(MCC_test_external_stack_ersilia, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardiotox Net Vs Ersilia Model Hub (eos2ta5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|    Data    |           Method           |  MCC   |  NPV   |  ACC   |  PPV   |  SPE   |  SEN   | B-ACC  |\n",
      "+------------+----------------------------+--------+--------+--------+--------+--------+--------+--------+\n",
      "| Test Set 1 |       Cardiotox Net        | 0.5994 | 0.6875 | 0.8095 | 0.8929 | 0.7857 | 0.8333 | 0.8095 |\n",
      "| Test Set 1 | Ersilia Model Hub(eos2ta5) | 0.5994 | 0.6875 | 0.8095 | 0.8929 | 0.7857 | 0.8333 | 0.8095 |\n",
      "+------------+----------------------------+--------+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "comparison_table = PrettyTable(['Data', \"Method\", \"MCC\",\"NPV\", \"ACC\", \"PPV\", \"SPE\", \"SEN\", \"B-ACC\"])\n",
    "comparison_table.add_row([\"Test Set 1\", \"Cardiotox Net\", 0.5994, 0.6875, 0.8095, 0.8929, 0.7857, 0.8333, 0.8095])\n",
    "comparison_table.add_row([\"Test Set 1\", \"Ersilia Model Hub(eos2ta5)\", 0.5994, 0.6875, 0.8095, 0.8929, 0.7857, 0.8333, 0.8095])\n",
    "print(comparison_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardiotox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
